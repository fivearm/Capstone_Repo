{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost, GradientBoost, and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score, plot_confusion_matrix, confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Data/X_train.csv', index_col='building_id')\n",
    "X_test = pd.read_csv('Data/X_test.csv', index_col='building_id')\n",
    "y_train = pd.read_csv('Data/y_train.csv', index_col='building_id')\n",
    "y_test = pd.read_csv('Data/y_test.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary and helper function to track model scores.\n",
    "score_dict = {}\n",
    "def print_scores():\n",
    "    for key in score_dict.keys():\n",
    "        print(f'{key},\\t f1_micro_score: {round(score_dict[key][0],4)},\\t Run time: {round(score_dict[key][1],0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide Columns and drop unimportant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by dropping unimportant features and using target encdoing, which was our best approach with the Random Forest models.  We'll try untuned AdaBoost, GradientBoosting, and XGBoost, and see which performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = []\n",
    "for col in X_train.columns:\n",
    "    if col.startswith('has'):\n",
    "        binary_cols.append(col)\n",
    "\n",
    "cat_cols = list(X_train.select_dtypes(include='object').columns)\n",
    "\n",
    "integer_cols = ['count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage', 'count_families']\n",
    "\n",
    "geo_cols = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']\n",
    "\n",
    "all_cols = geo_cols + cat_cols + integer_cols + binary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols_dropped = binary_cols.copy()\n",
    "for col in binary_cols_dropped:\n",
    "    if col.startswith('has_secondary'):\n",
    "        binary_cols_dropped.remove(col)\n",
    "binary_cols_dropped.append('has_secondary_use')\n",
    "\n",
    "cat_cols_dropped = cat_cols.copy()\n",
    "cat_cols_dropped.remove('legal_ownership_status')\n",
    "cat_cols_dropped.remove('plan_configuration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_pipe = imbPipeline([('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))])\n",
    "target_pipe = imbPipeline([('target', TargetEncoder(cols=geo_cols))])\n",
    "\n",
    "transformer = ColumnTransformer([\n",
    "    ('binary', 'passthrough', binary_cols_dropped),\n",
    "    ('categorical', ohe_pipe, cat_cols_dropped),\n",
    "    ('integer', 'passthrough', integer_cols),\n",
    "    ('geo', target_pipe, geo_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost0_pipe = imbPipeline([\n",
    "    ('transformer', transformer),\n",
    "    ('adaboost', AdaBoostClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 75.27919721603394\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "f1_micro_adaboost0 = cross_val_score(adaboost0_pipe, X_train, y_train, scoring='f1_micro')\n",
    "end = time.time()\n",
    "print(f'Run time: {end-start}')\n",
    "run_time_adaboost0 = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.718096699923254"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_micro_adaboost0 = f1_micro_adaboost0.mean()\n",
    "f1_micro_adaboost0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaboost0,\t f1_micro_score: 0.7181,\t Run time: 75.0\n"
     ]
    }
   ],
   "source": [
    "score_dict['adaboost0'] = [f1_micro_adaboost0, run_time_adaboost0]\n",
    "print_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest performs better than this model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientboost0_pipe = imbPipeline([\n",
    "    ('transformer', transformer),\n",
    "    ('gradient_boost', GradientBoostingClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 793.0880649089813\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "f1_micro_gradientboost0 = cross_val_score(gradientboost0_pipe, X_train, y_train, scoring='f1_micro')\n",
    "end = time.time()\n",
    "print(f'Run time: {end-start}')\n",
    "run_time_gradientboost0 = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7306369915579431"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_micro_gradientboost0 = f1_micro_gradientboost0.mean()\n",
    "f1_micro_gradientboost0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaboost0,\t f1_micro_score: 0.7181,\t Run time: 75.0\n",
      "gradientboost0,\t f1_micro_score: 0.7306,\t Run time: 793.0\n"
     ]
    }
   ],
   "source": [
    "score_dict['gradientboost0'] = [f1_micro_gradientboost0, run_time_gradientboost0]\n",
    "print_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any improvment over AdaBoost, but at a significant runtime cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost0_pipe = imbPipeline([\n",
    "    ('transformer', transformer),\n",
    "    ('xgboost', XGBClassifier(n_jobs=-1, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 332.93079018592834\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "f1_micro_XGBoost0 = cross_val_score(XGBoost0_pipe, X_train, y_train, scoring='f1_micro')\n",
    "end = time.time()\n",
    "print(f'Run time: {end-start}')\n",
    "run_time_XGBoost0 = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7375236633410078"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_micro_XGBoost0 = f1_micro_XGBoost0.mean()\n",
    "f1_micro_XGBoost0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaboost0,\t f1_micro_score: 0.7181,\t Run time: 75.0\n",
      "gradientboost0,\t f1_micro_score: 0.7306,\t Run time: 793.0\n",
      "XGBoost0,\t f1_micro_score: 0.7375,\t Run time: 333.0\n"
     ]
    }
   ],
   "source": [
    "score_dict['XGBoost0'] = [f1_micro_XGBoost0, run_time_XGBoost0]\n",
    "print_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is the best performer, outscoring GradientBoost in less than half the time.  Let's move on to tuning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try using a smaller train set for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample, X_train_leftover, y_train_sample, y_train_leftover = \\\n",
    "    train_test_split(X_train, y_train, test_size=.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_sample_pipe = imbPipeline([\n",
    "    ('transformer', transformer),\n",
    "    ('xgboost', XGBClassifier(n_jobs=-1, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 64.49462294578552\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "f1_micro_XGBoost_sample = cross_val_score(XGBoost_sample_pipe, X_train_sample, y_train_sample, scoring='f1_micro')\n",
    "end = time.time()\n",
    "print(f'Run time: {end-start}')\n",
    "run_time_XGBoost_sample = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.683141468406242"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_micro_XGBoost_sample = f1_micro_XGBoost_sample.mean()\n",
    "f1_micro_XGBoost_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaboost0,\t f1_micro_score: 0.7181,\t Run time: 75.0\n",
      "gradientboost0,\t f1_micro_score: 0.7306,\t Run time: 793.0\n",
      "XGBoost0,\t f1_micro_score: 0.7375,\t Run time: 333.0\n",
      "XGBoost1,\t f1_micro_score: 0.6831,\t Run time: 61.0\n",
      "XGBoost_sample,\t f1_micro_score: 0.6831,\t Run time: 64.0\n"
     ]
    }
   ],
   "source": [
    "score_dict['XGBoost_sample'] = [f1_micro_XGBoost_sample, run_time_XGBoost_sample]\n",
    "print_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 30.6min\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "XGBoost_param_grid = {'xgboost__max_depth': [3, 6, 10],\n",
    "                      'xgboost__learning_rate': [.01, .05, .3, .4],\n",
    "                      'xgboost__subsample': [.3, .5, 1]}\n",
    "\n",
    "XGBoost_gs = GridSearchCV(estimator=XGBoost_sample_pipe, param_grid=XGBoost_param_grid, \n",
    "                          scoring='f1_micro', cv=3, n_jobs=-1, verbose=2)\n",
    "XGBoost_gs.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(f'Run time: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
