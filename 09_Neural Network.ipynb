{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.metrics import f1_score, plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Data/X_train.csv', index_col='building_id')\n",
    "X_test = pd.read_csv('Data/X_test.csv', index_col='building_id')\n",
    "y_train = pd.read_csv('Data/y_train.csv', index_col='building_id')\n",
    "y_test = pd.read_csv('Data/y_test.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape y_train for the Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need y_train in the shape (len(y_train), 3).  I also need to re-encode y so that 1, 2, 3 goes to 0, 1, 2, so that I can use np.utils.to_categorical.  When I do my predictions, I will reverse this encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, ..., 2, 3, 2]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['target'] = y_train.damage_grade.apply(lambda x: x - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_y_train = np_utils.to_categorical(y_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195450, 3)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be doing the ususal transformations:  dropping unimportant columns, log-transforming and scaling the integer columns, and target encoding the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = []\n",
    "for col in X_train.columns:\n",
    "    if col.startswith('has'):\n",
    "        binary_cols.append(col)\n",
    "\n",
    "cat_cols = list(X_train.select_dtypes(include='object').columns)\n",
    "\n",
    "integer_cols = ['count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage', 'count_families']\n",
    "\n",
    "geo_cols = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']\n",
    "\n",
    "all_cols = geo_cols + cat_cols + integer_cols + binary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols_dropped = binary_cols.copy()\n",
    "for col in binary_cols_dropped:\n",
    "    if col.startswith('has_secondary'):\n",
    "        binary_cols_dropped.remove(col)\n",
    "binary_cols_dropped.append('has_secondary_use')\n",
    "\n",
    "cat_cols_dropped = cat_cols.copy()\n",
    "cat_cols_dropped.remove('legal_ownership_status')\n",
    "cat_cols_dropped.remove('plan_configuration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(x):\n",
    "    return np.log(x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_transformer = FunctionTransformer(log_transform)\n",
    "\n",
    "ohe_pipe = imbPipeline([('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))])\n",
    "integer_pipe = imbPipeline([\n",
    "    ('function', function_transformer),\n",
    "    ('ss', StandardScaler())\n",
    "])\n",
    "target_pipe = imbPipeline([('target', TargetEncoder(cols=geo_cols))])\n",
    "\n",
    "transformer = ColumnTransformer([\n",
    "    ('binary', 'passthrough', binary_cols_dropped),\n",
    "    ('categorical', ohe_pipe, cat_cols_dropped),\n",
    "    ('geo', target_pipe, geo_cols),\n",
    "    ('integer', integer_pipe, integer_cols),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_train = transformer.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195450, 49)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final X_train has 49 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras0 = Sequential()\n",
    "keras0.add(Dense(64, input_dim=49, activation='relu'))\n",
    "keras0.add(Dense(32, activation='relu'))\n",
    "keras0.add(Dense(16, activation='relu'))\n",
    "keras0.add(Dense(3, activation='softmax'))\n",
    "\n",
    "keras0.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 0s 2ms/step - loss: 0.6787 - accuracy: 0.6905\n",
      "Runtime: 0.7515280246734619 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "keras0.fit(transformed_X_train, dummy_y_train, epochs=1, batch_size=1000)\n",
    "end = time.time()\n",
    "print(f'Runtime: {end-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a cross validation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cross_val(name, X=transformed_X_train, y=dummy_y_train, epochs=1,\\\n",
    "                     batch_size=1000, n_splits=5, shuffle=True):\n",
    "    \n",
    "    \"\"\"Takes in a model and performs a cross-validation and returns the average micro-averaged F1 score.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    name: the name of the model\n",
    "    X:  default is 'transformed_X_train'\n",
    "    y:  default is 'dummy_y_train\n",
    "    epochs: default is 1\n",
    "    batch_size: default is 1000\n",
    "    n_splits: number of splits in the KFold, default is 5\n",
    "    shuffle:  whether to shuffle the splits, default is True\n",
    "    \"\"\"\n",
    "    \n",
    "    #Create a list to hold f1-micro scores\n",
    "    f1_micro_scores = []\n",
    "    #Instantiate a KFold object\n",
    "    kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=42)\n",
    "    \n",
    "    for train_ind, val_ind in kf.split(X):\n",
    "        #Fit the model\n",
    "        name.fit(X[train_ind], y[train_ind], epochs=epochs, batch_size=batch_size)\n",
    "        #Make predictions\n",
    "        y_pred_adj = name.predict_classes(X[val_ind])\n",
    "        #Calculate the f1-micro score\n",
    "        f1_micro = f1_score(np.argmax(y[val_ind], axis=1), y_pred_adj, average='micro')\n",
    "        print(f1_micro)\n",
    "        #Append the score to the list \n",
    "        f1_micro_scores.append(f1_micro)\n",
    "        \n",
    "    return np.mean(f1_micro_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 0.5604 - accuracy: 0.7503\n",
      "0.7533128677411103\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.7512\n",
      "0.74778715784088\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.5617 - accuracy: 0.7497\n",
      "0.7520081862368893\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.7511\n",
      "0.7518802762854949\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7515\n",
      "0.7466103862880532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7503197748784856"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_cross_val(keras0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate a KFold object and \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 0.5620 - accuracy: 0.7496\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.5613 - accuracy: 0.7501\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.5626 - accuracy: 0.7497\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.5617 - accuracy: 0.7493\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.5606 - accuracy: 0.7510\n",
      "Run time: 4.622860908508301\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "f1_micro_keras0 = []\n",
    "for train_ind, val_ind in kf.split(transformed_X_train):\n",
    "    #Fit the model\n",
    "    keras0.fit(transformed_X_train[train_ind], dummy_y_train[train_ind], epochs=1, batch_size=1000)\n",
    "    #Make predictions\n",
    "    y_pred_adj = keras0.predict_classes(transformed_X_train[val_ind])\n",
    "    #Calculate the f1-micro score\n",
    "    f1_micro = f1_score(np.argmax(dummy_y_train[val_ind], axis=1), y_pred_adj, average='micro')\n",
    "    #Append the score to the list \n",
    "    f1_micro_keras0.append(f1_micro)\n",
    "end = time.time()\n",
    "print(f'Run time: {end-start}')\n",
    "run_time_keras0 = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7502327961115375"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_micro_keras0 = np.mean(f1_micro_keras0)\n",
    "f1_micro_keras0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_test = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on X_test and score the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After predicting classes, I will reverse transform the predictions so that 0, 1, 2 is mapped to 1, 2, 3 (the original target.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = keras0.predict_classes(transformed_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred, columns=['y_pred_adj'], index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df['y_pred'] = y_pred_df['y_pred_adj'].apply(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7317309020582954"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_score = f1_score(y_test, y_pred_df.y_pred, average='micro')\n",
    "keras_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
