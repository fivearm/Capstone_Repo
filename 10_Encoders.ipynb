{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score, plot_confusion_matrix, confusion_matrix\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Data/X_train.csv', index_col='building_id')\n",
    "X_test = pd.read_csv('Data/X_test.csv', index_col='building_id')\n",
    "y_train = pd.read_csv('Data/y_train.csv', index_col='building_id')\n",
    "y_test = pd.read_csv('Data/y_test.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary and helper function to track model scores.\n",
    "score_dict = {}\n",
    "def print_scores():\n",
    "    for key in score_dict.keys():\n",
    "        print(f'{key},\\t f1_micro_score: {round(score_dict[key][0],4)},\\t Run time: {round(score_dict[key][1],0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide Columns and drop unimportant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by dropping unimportant features and using target encdoing, which was our best approach with the Random Forest models.  Then we'll try untuned AdaBoost, GradientBoosting, LightGBM, and XGBoost, and see which performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = []\n",
    "for col in X_train.columns:\n",
    "    if col.startswith('has'):\n",
    "        binary_cols.append(col)\n",
    "\n",
    "cat_cols = list(X_train.select_dtypes(include='object').columns)\n",
    "\n",
    "integer_cols = ['count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage', 'count_families']\n",
    "\n",
    "geo_cols = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']\n",
    "\n",
    "all_cols = geo_cols + cat_cols + integer_cols + binary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols_dropped = binary_cols.copy()\n",
    "for col in binary_cols_dropped:\n",
    "    if col.startswith('has_secondary'):\n",
    "        binary_cols_dropped.remove(col)\n",
    "binary_cols_dropped.append('has_secondary_use')\n",
    "\n",
    "cat_cols_dropped = cat_cols.copy()\n",
    "cat_cols_dropped.remove('legal_ownership_status')\n",
    "cat_cols_dropped.remove('plan_configuration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_pipe = imbPipeline([('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))])\n",
    "target_pipe = imbPipeline([('target', TargetEncoder(cols=geo_cols))])\n",
    "\n",
    "transformer = ColumnTransformer([\n",
    "    ('binary', 'passthrough', binary_cols_dropped),\n",
    "    ('categorical', ohe_pipe, cat_cols_dropped),\n",
    "    ('integer', 'passthrough', integer_cols),\n",
    "    ('geo', target_pipe, geo_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score, plot_confusion_matrix, confusion_matrix\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train = pd.read_csv('Data/X_train.csv', index_col='building_id')\n",
    "X_test = pd.read_csv('Data/X_test.csv', index_col='building_id')\n",
    "y_train = pd.read_csv('Data/y_train.csv', index_col='building_id')\n",
    "y_test = pd.read_csv('Data/y_test.csv', index_col='building_id')\n",
    "\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "#Create a dictionary and helper function to track model scores.\n",
    "score_dict = {}\n",
    "def print_scores():\n",
    "    for key in score_dict.keys():\n",
    "        print(f'{key},\\t f1_micro_score: {round(score_dict[key][0],4)},\\t Run time: {round(score_dict[key][1],0)}')\n",
    "\n",
    "### Divide Columns and drop unimportant features\n",
    "\n",
    "Let's start by dropping unimportant features and using target encdoing, which was our best approach with the Random Forest models.  Then we'll try untuned AdaBoost, GradientBoosting, LightGBM, and XGBoost, and see which performs best.\n",
    "\n",
    "binary_cols = []\n",
    "for col in X_train.columns:\n",
    "    if col.startswith('has'):\n",
    "        binary_cols.append(col)\n",
    "\n",
    "cat_cols = list(X_train.select_dtypes(include='object').columns)\n",
    "\n",
    "integer_cols = ['count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage', 'count_families']\n",
    "\n",
    "geo_cols = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']\n",
    "\n",
    "all_cols = geo_cols + cat_cols + integer_cols + binary_cols\n",
    "\n",
    "binary_cols_dropped = binary_cols.copy()\n",
    "for col in binary_cols_dropped:\n",
    "    if col.startswith('has_secondary'):\n",
    "        binary_cols_dropped.remove(col)\n",
    "binary_cols_dropped.append('has_secondary_use')\n",
    "\n",
    "cat_cols_dropped = cat_cols.copy()\n",
    "cat_cols_dropped.remove('legal_ownership_status')\n",
    "cat_cols_dropped.remove('plan_configuration')\n",
    "\n",
    "### Create the ColumnTransformer\n",
    "\n",
    "ohe_pipe = imbPipeline([('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))])\n",
    "target_pipe = imbPipeline([('target', TargetEncoder(cols=geo_cols))])\n",
    "\n",
    "transformer = ColumnTransformer([\n",
    "    ('binary', 'passthrough', binary_cols_dropped),\n",
    "    ('categorical', ohe_pipe, cat_cols_dropped),\n",
    "    ('integer', 'passthrough', integer_cols),\n",
    "    ('geo', target_pipe, geo_cols)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
